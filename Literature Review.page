---
format: markdown
title: Literature Review: The Pedagogy and Evaluation
       of Learning Activities
       Using Software Unit Tests
       in a Virtual Learning Environment
categories: draft report
...

> While there is a great deal of research being accomplished on “teaching machines”, many computer educators have not realized that when teaching the use of the computer they have access to the finest “teaching machine” of all—the digital computer.

&mdash; (Alan J. Perlis, editorial comment on Hollingsworth, 1960)

# Unit-Test Based Assessment of Programming

Unit tests have been a valuable pedagogical tool for more than 50 years. Despite their limitations, they enable tutors to set more formative assessments, and students value the rapid feedback.

The moment when a student starts writing their own unit tests is an important milestone in their development as programmers.

## History

Douce _at al_ (2006) divide programming assessment systems into three generations, and Ihantolo _et al_ (2010) hint at the emergence of a fourth which I will expand on in this report:

1. Pedagogically modified versions of operating systems and compilers; focused on efficient use of computer time.

2. Tools that add pedagogical functions to standard software development systems; focused on efficient use of the tutor's time and rapid feedback.

3. Web-based front-ends to second generation tools; focused on efficient deployment and rapid reconfiguration.

4. Virtual Learning Environment modules; focussed on efficient integration with institutional content delivery and administrative systems.


### First Generation Systems

The earliest systems (Hollingworth, 1960) were intended to get the best use out of scarce computer resources, but the implementors recognised other benefits:

* Students could work more efficiently, because they could work independently, and at their own pace.

* Tutors could work more efficiently, because the system enabled larger classes, and reduced the amount of tutor time needed to assess student's work.

* Students could mail in their code, so distance learning became.

They also recognised problems which will be familiar to modern implementors:

* Students treat violating the system's integrity as a legitimate exercise.

* Facilities were needed to handle code that consumed too many system resources, or never terminated.  (In the earliest implementations, these were human operators.)

* The nature of the activities left little room for creative exploration.

### Second Generation Systems

Frequent formative assessments are valuable, but checking student submissions for logical errors is error-prone, tedious and time-consuming. As the cost of computer equipment fell, the tutors became the bottleneck.  Automated tutors relieved tutors of much of the burden, freeing them to check for matters like coding style and maintainability. Students also appreciated the rapid feedback. (Isaacson and Scott, 1989)

### Third Generation Systems

The advantages of deploying software on the web are now widely recognised (see, for example, Graham, 2010).

Delivery from a central web server (which is also a feature of some second generation systems), allows:

* reduced development costs through the use of standard web technologies

* easy access from a standard cross-platform web browser

* rapid reconfiguration, and setting of new assignments
* automated plagiarism detection

 (Cheang *et al*, 2003) 

### Fourth Generation Systems

Deployment as a module in an institutional's learning environment allows:

* integrating programming exercises with course content

* simplified administration

* integration with the university's other assignment setting and grading systems

* reduced costs through eliminated duplicate functions

(Britain and Liber, 2004)

## Pedagogy

Unit tests have their limitations: they restrict the kinds of assignments that can be set, and the kinds of marking policies that can be operated. Humans must still assess code maintainability, but this need not be very time-consuming.

Students are generally happy with the idea of using automated assessment, because they value the rapid feedback. The kind of feedback given is an important pedagogical decision.

This form of assessment is also an excellent introduction to the Test Driven Development methodology. An interesting development is to have the students write the unit tests themselves, and assess the quality of the unit tests.

### Limitations on what can be marked

We want our students to produce code that:

* does not degrade system performance with poorly chosen algorithms
* follows sensible conventions
* is maintainable
* does not 'reinvent the wheel'

Algorithmic performance can be checked for by monitoring running time and memory usage during unit tests, and source code checkers can check for adherence to conventions.  The rest require human assessment, but this need not be time-consuming: if an assessor is having difficulty reading the code, they can give it a low maintainability score and move on.

### Limitations on what kinds of assignments can be set

Unit tests require a well defined interface, and outputs that can easily be assessed for correctness. Douce _et al._ (2006) illustrate the problems that can arise by discussing an assignment to generate an image of the Union Flag (the national flag of the UK).  Writing a test that can tolerate small deviations is difficult; specifying the requirements so precisely that no deviations are possible changes the nature of the problem.

### Limitations on Marking Policies

In some circumstances, human markers choose to overlook small slips and mistakes. Automatic markers cannot make that judgment, and students may become demoralised by this harsh treatment.

This can be mitigated if resubmissions are permitted, or if students are allowed to run the unit tests themselves before they submit.

### Feedback

Choices range from no feedback whatsoever (students just upload the code and cross their feedback) to direct access to the test cases.

In the latter case, students may cheat by writing code that simply detects which unit test is being run and supplies the expected response.

### Meta-testing

Modern programmers view unit tests as essential to sound engineering practice; some even advocate writing the tests before the implementation.

For the student, writing an effective suite of unit tests &ndash; especially *negative tests* &ndash; is an important conceptual development.

Unit tests can, themselves, be assessed, by running them against an instrumented reference implementation and checking for code coverage.  (Edwards, 2003; 2004)

# Pedagogical Challenges of Computer Programming

Despite the best efforts of all involved, a high proportion of Computer Science students do not become effective programmers.

The causes of this problem are a matter of ongoing research, but the failure to develop viable mental models of abstractions seems to be involved. Computer programs are abstract in nature, so perhaps this should come as no surprise that difficulties with abstract thought.

Other work shows that students display striking differences in their conceptions of the act of learning to program. Given the amount of independent study and practice required to achieve competence, it is reasonable to suspect that these differences in conception will lead to different outcomes.

## Computer Science courses can produce inept programmers

> Do students in introductory computing courses  know how to program at the expected skill level? The results from this trial assessment provide the answer  “No!” and suggest that the problem is fairly universal.  
(McCracken _et al._, 2001)

Reports from industry show that this is not just a problem for beginners: many graduates and even post-graduates fail basic questions.

> A surprisingly large fraction of applicants, even those with masters' degrees and PhDs in computer science, fail during interviews when asked to carry out basic programming tasks. For example, I've personally interviewed graduates who can't answer "Write a loop that counts from 1 to 10"
> (Kegel, 2004, cited in Atwood 2012)

## Studies of Mental Modelling

Bowden _et al._ (1992) found that, even after completing a first undergraduate course in Physics in which they learnt to accurately model the behaviour of mechanical systems, students still retained distinctly Aristotelian intuitions about the nature of motion. They did not seem to have made the connection between the signed and the signifier. Similar observations have been made in many other disciplines. (Laurillard, 2002)

Booth (1992) elicited mental models of recursion from students learning a fuctional programming language. She identified three distinct models:

1. Recursion as a syntactic feature of the language.

2. Recursion as a means of bringing about repetition in a functional language.

3. Recursion as self-reference, with a non-self-referential base case.

Only the third model is viable.

This is of more than just academic importance.  Kegel (2004, cited in Atwood, 2012) complains:

> I've interviewed many candidates who can't use recursion to solve a real problem. These are basic skills.

Dehnadi _et al._ (2009) show that a test of consistency of mental modelling is the best available predictor of performance in a programming course.

## Conceptions of the Act of Learning to Program

Bruce _et al_ (2006) and Stoodley _et al_ (2006)  interviewed students with widely differing backgrounds and levels of experience taking undergraduate and graduate courses in order to elicit their conceptions of the act of learning to program. They were able to divide their students into five categories:

1. *Following* &ndash; "I just want to get enough marks to pass the course"

2. *Coding* &ndash; "I need to memorise the syntax of the language"

3. *Understanding and Integrating* &ndash; "I want to know *why*, I want to understand"

4. *Problem solving* &ndash; "I want to learn how to use this language to get things done"

5. *Participating* &ndash; "I want to enter into the community of professional programmers"

6. *Satisfying clients* &ndash; "I want to learn how to use this language to satisfy business needs"

The difference between Categories 2 and 3 is striking. A Category 2 student sees time spent on difficult-to-grasp concepts that are not directly expressed in the language as time wasted. A Category 3 student sees time spent on the minutiae of the language as time wasted. This may reflect the distinction between *shallow* and *deep* learners identified by Säljö (1982).

They did not investigate the relation between student conception and learning outcomes.

# Pedagogical Evaluation of Virtual Learning Environments

In their report, Britain and Liver (2004) note the widespread adoption of Virtual Learning Environments in schools and universities, and see encouraging signs of a trend towards adopting more flexible, modular systems.

They present a framework designed for institutions evaluation whole VLEs, so much of it is irrelevant to this project, but their work is, in part, based on the Conversation Framework (Laurillard, 2002), so it serves as an example of how to develop an explicit set of requirements from the Conversation Framework.

## The Revised Pedagogical Framework

The following questions are the parts of Britain and Liver's framework that is most relevant to my work:

 1. What tools does the system provide for teachers to present/express their ideas to students?

 2. What tools does the system provide for students to articulate their ideas to teachers and other students?

 3. Can teachers and learners extend/change their presentations during the module’s time period?

 4. A VLE is not a single tool; it is a structuring and coordination system containing a variety of tools. These questions are about the model of teaching and learning interactions that forms the basis of the system.

     *   Can a module be structured sequentially and / or hierarchically over time?

     *  What facilities are there to organise learners in a variety of ways in the module (whole group, small groups, individuals)?

     *  What types of learning activity are supported by the system?
 
     *  What underlying pedagogical model(s) or approach(es) does the system encourage?

5. How are the ‘rules of the module’ expressed and made evident to the student? By this we mean such things as the learning outcomes, the obligations of the learner and the mutual commitment teacher and student make (e.g. the amount of time a teacher will spend messaging each week, the number of assignments a learner will be expected to complete, etc.)

6. What facilities are there to monitor how well learning is progressing on the module?

7. What can learners do on their own, outside of the purview of the teacher?

 * Can they find and manage resources – do they have their own file stores or repositories?

 * Can they talk to other students (other than in the main module discussion), create their own discussions, create their own learning activities involving peers?

8. To what extent is it possible for the teacher to adapt the module structure once teaching is underway?

     *  Can you add / change / delete resources?

     *  Can you add / change / delete fragments of module structure
 
     *  Can you add / remove people? Can you split them into different groups?

     *  Can you create and assign resources or learning activities to individuals?

# Designing e-Learning Tools


----------------

REPLs
-----

The Conversational Framework
----------------------------

Laurillard's notion of teaching is essentially rhetorical.
Of course, it would be great if students could go out and explore the subject entirely independently,
but time is limited, and there is an urgent need to get the students aligned with the necessary foundation materials.
However, Laurillard explicitly rejects the transmission model of teaching.
She prefers a rhetorical approach, in which the teacher brings the students round to a new relation with the world.
This is especially important for academic learning because, almost by definition,
academic knowledge is not accessible through direct experience.
It is a second-order experience of the world, which must be mediated through the teacher.

According to Laurillard, constructivism is old and busted[^problems-with-constructivism]
and phenomenography is the new hotness.

[^problems-with-constructivism]:
    Although constructivism has much to recommend it --
    especially its rejection of the transmission model of learning --
    it lacks the evidential basis required to make useful,
    student-centred prescriptions.

Phenomenography is a method of discovery (rather than hypothesis testing)
that produces qualitative data about categories of experience.
Pedagogically, phenomenography can show us:

 *  how students’ and teachers’ conceptions of the subject matter differ,
    and how we might make those conceptions explicit, so they may be usefully compared and contrasted

 *  the characteristics of successful and unsuccessful student–teacher–subject matter interactions

It does not -- indeed **cannot** -- show us how to present a particular conception to a student,
but it does suggest how teachers and students' conceptions can be made accessible to one another
and the differences highlighted.

Another important aspect is mapping between different representations of the same concepts.
For instance, it is often observed that students who can successfully answer quite difficult questions about Newtonian mechanics
still maintain distinctly Aristotelian intuitions.
This might be attributed to students' lack of practice
in mapping between mathematical abstractions and their intuitions about the physical world.

> The teaching strategy has been refined into a set of requirements for any learning situation:
> 
>  *  it must operate as an iterative dialogue;
>  *  which must be discursive, adaptive, interactive and reflective;
>  *  and which must operate at the level of descriptions of the topic;
>  *  and at the level of actions within related tasks.