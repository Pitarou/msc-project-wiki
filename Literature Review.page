---
format: markdown
title: Literature Review: The Pedagogy and Evaluation
       of Learning Activities
       Using Software Unit Tests
       in a Virtual Learning Environment
categories: draft report
...

I am currently using [Gingko](https://gingkoapp.com/#3551736a0d076c99a1000015) to organise my notes and ideas.
It probably makes more sense when viewed there.

--------------
> While there is a great deal of research being accomplished on “teaching machines”, many computer educators have not realized that when teaching the use of the computer they have access to the finest “teaching machine” of all

&mdash; (Alan J. Perlis, editorial comment on Hollingsworth, 1960)

# Unit-Test Based Assessment of Programming

Unit tests have been a valuable pedagogical tool for more than 50 years. Despite their limitations, they enable tutors to set more formative assessments, and students value the rapid feedback.  The moment when a student starts writing their own unit tests is an important milestone in their development as programmers.

## History

Douce _at al_ (2006) divide programming assessment systems into three generations, and Ihantolo _et al_ (2010) hint at the emergence of a fourth which I will expand on in this report:

1. Pedagogically modified versions of operating systems and compilers; focused on efficient use of computer time.

2. Tools that add pedagogical functions to standard software development systems; focused on efficient use of the tutor's time and rapid feedback.

3. Web-based front-ends to second generation tools; focused on efficient deployment and rapid reconfiguration.

4. Virtual Learning Environment modules; focussed on efficient integration with institutional content delivery and administrative systems.


### First Generation Systems

The earliest systems (Hollingworth, 1960) were intended to get the best use out of scarce computer resources, but the implementors recognised other benefits:

* Students could work more efficiently, because they could work independently, and at their own pace.

* Tutors could work more efficiently, because the system enabled larger classes, and reduced the amount of tutor time needed to assess student's work.

* Students could mail in their code, so distance learning became.

They also recognised problems which will be familiar to modern implementors:

* Students treat violating the system's integrity as a legitimate exercise.

* Facilities were needed to handle code that consumed too many system resources, or never terminated.  (In the earliest implementations, these were human operators.)

* The nature of the activities left little room for creative exploration.

### Second Generation Systems

Frequent formative assessments are valuable, but checking student submissions for logical errors is error-prone, tedious and time-consuming. As the cost of computer equipment fell, the tutors became the bottleneck.  Automated tutors relieved tutors of much of the burden, freeing them to check for matters like coding style and maintainability. Students also appreciated the rapid feedback. (Isaacson and Scott, 1989)

### Third Generation Systems

The advantages of deploying software on the web are now widely recognised (see, for example, Graham, 2010).

Delivery from a central web server (which is also a feature of some second generation systems), allows:

* reduced development costs through the use of standard web technologies

* easy access from a standard cross-platform web browser

* rapid reconfiguration, and setting of new assignments
* automated plagiarism detection

 (Cheang *et al*, 2003) 

### Fourth Generation Systems

Deployment as a module in an institutional's learning environment allows:

* integrating programming exercises with course content

* simplified administration

* integration with the university's other assignment setting and grading systems

* reduced costs through eliminated duplicate functions

(Britain and Liber, 2004)

## Pedagogy

Unit tests have their limitations: they restrict the kinds of assignments that can be set, and the kinds of marking policies that can be operated. Humans must still assess code maintainability, but this need not be very time-consuming.

Students are generally happy with the idea of using automated assessment, because they value the rapid feedback. The kind of feedback given is an important pedagogical decision.

This form of assessment is also an excellent introduction to the Test Driven Development methodology, and it is even possible to assess the quality of unit tests written by students.

### Limitations on what can be marked

We want our students to produce code that:

* does not degrade system performance with poorly chosen algorithms
* follows sensible conventions
* is maintainable
* does not 'reinvent the wheel'

Algorithmic performance can be checked for by monitoring running time and memory usage during unit tests, and source code checkers can check for adherence to conventions.  The rest require human assessment, but this need not be time-consuming: if an assessor is having difficulty reading the code, they can give it a low maintainability score and move on.

### Limitations on what kinds of assignments can be set

Unit tests require a well defined interface, and outputs that can easily be assessed for correctness. Douce _et al._ (2006) illustrate the problems that can arise by discussing an assignment to generate an image of the Union Flag (the national flag of the UK).  Writing a test that can tolerate small deviations is difficult; specifying the requirements so precisely that no deviations are possible changes the nature of the problem.

### Limitations on Marking Policies

In some circumstances, human markers choose to overlook small slips and mistakes. Automatic markers cannot make that judgement, and students may become demoralised by their harshness.  This can be mitigated if resubmissions are permitted, or if students are allowed to run the unit tests themselves before they submit.

### Feedback

Choices range from no feedback whatsoever (students just upload the code and cross their feedback) to direct access to the test cases.  In the latter case, students might cheat by writing code that simply detects which unit test is being run and supplies the expected response.

### Meta-testing

Modern programmers view unit tests as essential to sound engineering practice; some even advocate writing the tests before the implementation.  For the student, writing an effective suite of unit tests &ndash; especially *negative tests* &ndash; is an important conceptual development.  Unit tests can, themselves, be assessed, by running them against an instrumented reference implementation and checking code coverage.  (Edwards, 2003; 2004)

# The Problem of Teaching to Program

Despite the best efforts of all involved, a high proportion of Computer Science students do not become effective programmers. The causes of this problem are a matter of ongoing research, but the failure to develop viable mental models of abstractions seems to be involved. Other lines of enquiry show striking and suggestive differences in students' understanding of programming, but the literature does not support any firm conclusions.

## Computer Science Courses Do Not Produce Able Programmers

> Do students in introductory computing courses  know how to program at the expected skill level? The results from this trial assessment provide the answer  “No!” and suggest that the problem is fairly universal.  
(McCracken _et al._, 2001)

Reports from industry show that this problem persists past graduation.

> A surprisingly large fraction of applicants, even those with masters' degrees and PhDs in computer science, fail during interviews when asked to carry out basic programming tasks. For example, I've personally interviewed graduates who can't answer "Write a loop that counts from 1 to 10"  
> (Kegel, 2004, cited in Atwood 2012)

## Mental Models of Abstractions

Academic students often struggle with abstract models, perhaps because there is not enough emphasis on the connection between the model and the reality.  The abstract nature of software makes this failure to grasp abstractions an acute practical problem.

### Failure to Internalise Abstractions

Laurillard (2002) asserts that the essence of academic teaching is to impart descriptions of the world not accessible through direct experience (abstract models).  But in practice students often do not grasp the connection between the description and the reality.

For example, Bowden _et al._ (1992) found that students who had completed a first undergraduate course in Newtonian Physics often retained Aristotelian intuitions about the nature of motion.

### Shallow and Deep Learners

Säljö (1982) presented students with an academic text which described two contrasting models of learning, and illustrated them with anecdotes and examples. Some students, which he denoted "shallow learners" perceived only the linear narrative; only the "deep learners" could appreciate the structure of the underlying thesis.

### Failure to Acquire Viable Mental Models of Recursion

Booth (1992) elicited mental models of recursion from students learning a fuctional programming language. She identified three distinct models:

1. Recursion as a syntactic feature of the language.

2. Recursion as a means of bringing about repetition in a functional language.

3. Recursion as self-reference, with a non-self-referential base case.

Only the third model is complete enough to be of practical use.

### Practical Consequences for Computer Programmers of Failure to Internalise Abstractions

Kegel (2004, cited in Atwood, 2012) complains:

> I've interviewed many candidates who can't use recursion to solve a real problem. These are basic skills.

## Different Approaches to Learning

In a phenomonographic study, Bruce _et al_ (2006) and Stoodley _et al_ (2006) found striking differences in how students approach the act of learning to program.  But it should be noted that phenomonographic studies are entirely contextualised: they identify characteristics of the learner's approach to a particular task, not of the learner. (Laurillard, 2002)

### Conceptions of the Act of Learning to Program

Bruce _et al_ (2006) and Stoodley _et al_ (2006)  interviewed students with widely differing backgrounds and levels of experience taking undergraduate and graduate courses in order to elicit their conceptions of the act of learning to program. They found that their students fell into six categories:

1. *Following* &ndash; "I just want to get enough marks to pass the course"

2. *Coding* &ndash; "I want to learn the syntax of the language"

3. *Understanding and Integrating* &ndash; "I want to know *why*; I want to understand"

4. *Problem solving* &ndash; "I want to know *how*; I want to get things done"

5. *Participating* &ndash; "I want to be a professional programmers"

6. *Satisfying clients* &ndash; "I want to learn how to use this language to satisfy business needs"

The difference between Categories 2 and 3 is striking. A Category 2 student sees time spent on difficult-to-grasp concepts that are not directly expressed in the language as time wasted; a Category 3 student sees it as time well spent.

## Predictive Power of Consistent Mental Modelling

Dehnadi _et al._ (2009) found that a test of consistency of mental modelling was the most robust available predictor of performance in a programming course.

In an earlier unpublished paper, Dehnadi and Bornat (2006) also noted the bimodal distribution of final test scores, which suggests a qualitative difference between the successful and unsuccessful programmers.

## Further Research is Needed

It is tempting to conclude that Säljö, Bruce et al., Stoodley et al., and Dehnadi and Bornat are all looking at the same phenomenon of "deep" and "shallow" learning. But, as Laurillard points out, we do not know enough about the interaction between learner, context, task, and goal to draw any firm conclusions. Further research is very much needed.

# Pedagogical IDEs

## The Read--Eval--Print Loop (REPL)

# Mayes and Fowler's Framework For Understanding Courseware

Mayes and Fowler (1999) propose a three-level classification of courseware, based on a three-stage model for learning;

1. **Primary courseware** presents new content and ideas.

2. **Secondary courseware** builds understanding through meaningful tasks

3. **Tertiary courseware** allows students to test and refine their understanding through dialogue with reality (application), the tutor, other learners, or the self (reflection).

The authors were particularly interested in the unexplored possibilities of large-scale tertiary courseware.

In hindsight, they anticipated the importance of on-line Question & Answer fora such as Stack Exchange.

# The Conversational Framework

> The aim of teaching is to make student learning possible. 

Laurillard's (2002) framework describes the interactions necessary for teaching to succeed.  They are summarised in this diagram:

![The Conversational Framework](/The Conversational Framework.png)

Laurillard then goes on to classify the media in terms of the interactions they support.  From the principles of the framework, and from the expressive possibilities of the new electronic media, she argues for a new class of media which she calls *the productive media*.

Much later in the book:

> The most difficult part is designing the actions that have intrinsic feedback for comparison against target goals, as this is the most unfamiliar for academics.  ... Give them the interactive environment in which they perform the activities of the scholar but with feedback related to a goal in such a way that it exposes the internal relation to them, and makes it meaningful.

## Background to the Conversational Framework

 *  academic teaching is a rhetorical activity
 *  distinction between intrinsic and extrinsic feedback
 *  differences between teachers' and students' conception of the goals of learning activities
 *  lack of empirical knowledge about the interaction between context, goals, and learning outcomes
 *  weakness of Instructional Design
 *  weakness of Constructivist Psychology as a theoretical basis for teaching strategies
 * strength of the Phenomonographic approach

### The Rhetorical Nature of Academic Teaching

Academic knowledge is, by definition, second-order knowledge that is not accessible to direct experience.

> Teaching is essentially a rhetorical activity, seeking to persuade students to change the way they experience the world through an understanding of the insights of others.

### Intrinsic and Extrinsic Feedback

Intrinsic feedback is a natural consequence of the action.  For instance:

 *  if you tip a bucket, water will flow out faster
 *  if you move the mouse, the pointer on the screen moves
 *  if you mispronounce a word, people will tend to correct you

Extrinsic feedback is an external comment on an action.  In academic learning, extrinsic feedback is sometimes the only feedback we get.

### Weakness of Instructional Design

Although logical in its conception, Instructional Design had no empirical basis.  It was later enriched with results from psychological research, but these results were misapplied: experiments designed to elucidate the function of the brain do not necessarily tell us how to teach.

### Weakness of Constructivist Psychology

Although it has much to recommend it, it turns out that there are serious problems with the practical application of Constructivism to teaching.

Laurillard cannot find any constructivist prescriptions that are empirically based, student-centred, and applicable.  The person--world dualism of constructivism is also problematic.

### The Phenomenographic Approach

Phenomenography is a method for eliciting qualitative data about categories of description. It gives us access to:

 *  differences in teachers' and students' conceptions
 *  the characteristics of successfula nd unsuccessful student--teacher--subject matter interactions

It does not provide a prescription for teaching, but it does show us what is relevant.

### Conclusions of Phenomonographic Studies of Learning Activities

In a successful program of learning activities, a student should:

 *  understand the **structure** of the discourse (distinguish evidence from argument; example from model)
 *  map between different forms of representation
 *  act on descriptions of the world
 *  use feedback, both extrinsic and extrinsic, to adjust actions and conceptual descriptions to meet the goal
 *  reflect on the goal-action-feedback cycle

## Classifications of Interactions in the Conversation Framework

Arrows 1--4 are the *discursive* interactions, in which teachers and students describe and redescribe their conceptions and examine differences.

Arrows 5 and 10 are the *adaptive* interactions, in which student and teacher adapt their actions in the light of discussion.

Arrows 6--9 are the *interactive* interactions, in which students act towards goals and receive feedback in a task environment (or *micro-world*) provided by teacher.

Arrows 11 and 12 are the *reflective* interactions, in which the student re-examines their conceptions in light of what they observed in the goal--action--feedback cycle, and the teacher modifies their descriptions based on their observations of student actions.

### The Framework in Full

1. Teacher describes conceptions to the student
2. Student describes conceptions to the teacher
3. Teacher redescribes conceptions in terms of student’s conceptions or actions
4. Student redescribes conceptions in terms of teacher’s redescription or student’s actions
5. Teacher adapts tasks or goals in light of student’s description
6. Teacher sets a task goal
7. Student acts to achieve a task goal
8. Teacher sets up an environment to give intrinsic feedback on student's actions
9. Student modifies actions in light of feedback on actions
10. Student adapts actions in light of teacher’s description or student’s redescription
11. Student reflects internally on interactions to modify redescription
12. Teacher  reflects on student’s actions to modify redescription

## Productive Media



## Pedagogical Evaluation of Virtual Learning Environments

In their report, Britain and Liver (2004) note the widespread adoption of Virtual Learning Environments in schools and universities, and see encouraging signs of a trend towards adopting more flexible, modular systems.

They present a framework designed for institutions evaluating whole VLEs, so much of it is irrelevant to this project, but their work is, in part, based on the Conversation Framework, so it serves as an example of how to develop an explicit set of requirements from the Conversation Framework.

### The Revised Pedagogical Framework

The following questions are the parts of Britain and Liver's framework that is most relevant to my work:

 1. What tools does the system provide for teachers to present / express their ideas to students?

 2. What tools does the system provide for students to articulate their ideas to teachers and other students?

 3. Can teachers and learners extend / change their presentations during the module’s time period?

 4. A VLE is not a single tool; it is a structuring and coordination system containing a variety of tools. These questions are about the model of teaching and learning interactions that forms the basis of the system.

     *   Can a module be structured sequentially and / or hierarchically over time?

     *  What facilities are there to organise learners in a variety of ways in the module (whole group, small groups, individuals)?

     *  What types of learning activity are supported by the system?
 
     *  What underlying pedagogical model(s) or approach(es) does the system encourage?

5. How are the ‘rules of the module’ expressed and made evident to the student? By this we mean such things as the learning outcomes, the obligations of the learner and the mutual commitment teacher and student make (e.g. the amount of time a teacher will spend messaging each week, the number of assignments a learner will be expected to complete, etc.)

6. What facilities are there to monitor how well learning is progressing on the module?

7. What can learners do on their own, outside of the purview of the teacher?

 * Can they find and manage resources – do they have their own file stores or repositories?

 * Can they talk to other students (other than in the main module discussion), create their own discussions, create their own learning activities involving peers?

8. To what extent is it possible for the teacher to adapt the module structure once teaching is underway?

     *  Can you add / change / delete resources?

     *  Can you add / change / delete fragments of module structure
 
     *  Can you add / remove people? Can you split them into different groups?

     *  Can you create and assign resources or learning activities to individuals?